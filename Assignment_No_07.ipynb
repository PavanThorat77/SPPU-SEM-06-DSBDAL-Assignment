{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKLktP2bQaCe"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idqz6Ap3SZgs",
        "outputId": "d20d1bc2-63d6-494f-a33a-c1e24239b8cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_document = \"\"\"\n",
        "This is a sample document. It contains several sentences that need to be preprocessed.\n",
        "We will tokenize the text, remove stopwords, apply stemming and lemmatization, and calculate TF-IDF.\n",
        "\"\"\"\n",
        "\n",
        "sample_document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KO_ElMnnSdpW",
        "outputId": "a464be89-f9be-44de-ec6e-fd4ae5747805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis is a sample document. It contains several sentences that need to be preprocessed. \\nWe will tokenize the text, remove stopwords, apply stemming and lemmatization, and calculate TF-IDF.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(sample_document)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpXnxQzQSh42",
        "outputId": "17f10ab2-abd0-43c5-9e89-c8e87511efc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'sample',\n",
              " 'document',\n",
              " '.',\n",
              " 'It',\n",
              " 'contains',\n",
              " 'several',\n",
              " 'sentences',\n",
              " 'that',\n",
              " 'need',\n",
              " 'to',\n",
              " 'be',\n",
              " 'preprocessed',\n",
              " '.',\n",
              " 'We',\n",
              " 'will',\n",
              " 'tokenize',\n",
              " 'the',\n",
              " 'text',\n",
              " ',',\n",
              " 'remove',\n",
              " 'stopwords',\n",
              " ',',\n",
              " 'apply',\n",
              " 'stemming',\n",
              " 'and',\n",
              " 'lemmatization',\n",
              " ',',\n",
              " 'and',\n",
              " 'calculate',\n",
              " 'TF-IDF',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(tokens)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od2ixtDrSmC2",
        "outputId": "447912b1-3679-4201-a75c-0d224b66c3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('sample', 'JJ'),\n",
              " ('document', 'NN'),\n",
              " ('.', '.'),\n",
              " ('It', 'PRP'),\n",
              " ('contains', 'VBZ'),\n",
              " ('several', 'JJ'),\n",
              " ('sentences', 'NNS'),\n",
              " ('that', 'WDT'),\n",
              " ('need', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('be', 'VB'),\n",
              " ('preprocessed', 'VBN'),\n",
              " ('.', '.'),\n",
              " ('We', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('tokenize', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('text', 'NN'),\n",
              " (',', ','),\n",
              " ('remove', 'VB'),\n",
              " ('stopwords', 'NNS'),\n",
              " (',', ','),\n",
              " ('apply', 'VB'),\n",
              " ('stemming', 'VBG'),\n",
              " ('and', 'CC'),\n",
              " ('lemmatization', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('calculate', 'VB'),\n",
              " ('TF-IDF', 'NNP'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T9jtqOjSqvm",
        "outputId": "46d44517-3842-4517-deb3-704cdd2a8c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sample',\n",
              " 'document',\n",
              " '.',\n",
              " 'contains',\n",
              " 'several',\n",
              " 'sentences',\n",
              " 'need',\n",
              " 'preprocessed',\n",
              " '.',\n",
              " 'tokenize',\n",
              " 'text',\n",
              " ',',\n",
              " 'remove',\n",
              " 'stopwords',\n",
              " ',',\n",
              " 'apply',\n",
              " 'stemming',\n",
              " 'lemmatization',\n",
              " ',',\n",
              " 'calculate',\n",
              " 'TF-IDF',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "stemmed_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7euJhorqS32R",
        "outputId": "bbfe831c-fa85-4c1b-e7bd-da3c0c0539a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sampl',\n",
              " 'document',\n",
              " '.',\n",
              " 'contain',\n",
              " 'sever',\n",
              " 'sentenc',\n",
              " 'need',\n",
              " 'preprocess',\n",
              " '.',\n",
              " 'token',\n",
              " 'text',\n",
              " ',',\n",
              " 'remov',\n",
              " 'stopword',\n",
              " ',',\n",
              " 'appli',\n",
              " 'stem',\n",
              " 'lemmat',\n",
              " ',',\n",
              " 'calcul',\n",
              " 'tf-idf',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "lemmatized_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3EXabjcTUEX",
        "outputId": "41896dc8-c104-47e1-900c-4b2b45645b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sample',\n",
              " 'document',\n",
              " '.',\n",
              " 'contains',\n",
              " 'several',\n",
              " 'sentence',\n",
              " 'need',\n",
              " 'preprocessed',\n",
              " '.',\n",
              " 'tokenize',\n",
              " 'text',\n",
              " ',',\n",
              " 'remove',\n",
              " 'stopwords',\n",
              " ',',\n",
              " 'apply',\n",
              " 'stemming',\n",
              " 'lemmatization',\n",
              " ',',\n",
              " 'calculate',\n",
              " 'TF-IDF',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_tf_idf(documents):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    return tfidf_matrix, feature_names\n",
        "\n",
        "# Calculate TF-IDF\n",
        "documents = [sample_document]\n",
        "tfidf_matrix, feature_names = calculate_tf_idf(documents)\n",
        "\n",
        "# Print TF-IDF information\n",
        "tfidf_matrix.toarray(), feature_names\n"
      ],
      "metadata": {
        "id": "GiCp2NHbTl7g",
        "outputId": "784a135d-2454-4f5b-e7fc-827225d88cf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.36514837, 0.18257419, 0.18257419, 0.18257419, 0.18257419,\n",
              "         0.18257419, 0.18257419, 0.18257419, 0.18257419, 0.18257419,\n",
              "         0.18257419, 0.18257419, 0.18257419, 0.18257419, 0.18257419,\n",
              "         0.18257419, 0.18257419, 0.18257419, 0.18257419, 0.18257419,\n",
              "         0.18257419, 0.18257419, 0.18257419, 0.18257419, 0.18257419,\n",
              "         0.18257419, 0.18257419]]),\n",
              " array(['and', 'apply', 'be', 'calculate', 'contains', 'document', 'idf',\n",
              "        'is', 'it', 'lemmatization', 'need', 'preprocessed', 'remove',\n",
              "        'sample', 'sentences', 'several', 'stemming', 'stopwords', 'text',\n",
              "        'tf', 'that', 'the', 'this', 'to', 'tokenize', 'we', 'will'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}